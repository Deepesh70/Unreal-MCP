<p align="center">
  <h1 align="center">üéÆ Unreal MCP</h1>
  <p align="center">
    <strong>Control Unreal Engine with natural language using LLM agents and the Model Context Protocol.</strong>
  </p>
  <p align="center">
    <a href="#-quick-start">Quick Start</a> ‚Ä¢
    <a href="#-features">Features</a> ‚Ä¢
    <a href="#%EF%B8%8F-architecture">Architecture</a> ‚Ä¢
    <a href="#-supported-llm-backends">LLM Backends</a> ‚Ä¢
    <a href="#-available-tools">Tools</a> ‚Ä¢
    <a href="#-documentation">Docs</a>
  </p>
</p>

---

## üìñ Overview

**Unreal MCP** is a Python-based bridge that connects Large Language Models (LLMs) to Unreal Engine via the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). It lets you spawn actors, list objects, transform geometry, and more ‚Äî all through plain-English commands.

The system has two halves:

| Component | Description |
|-----------|-------------|
| **MCP Server** (`server.py`) | A FastMCP server that exposes Unreal Engine operations as MCP tools over SSE. It communicates with UE via WebSocket using the built-in Remote Control API. |
| **LLM Agent** (`agent.py`) | A LangChain-powered agent that connects to the MCP server, discovers available tools, and uses an LLM to plan & execute multi-step operations inside the Unreal Editor. |

---

## ‚ú® Features

- üó£Ô∏è **Natural Language Control** ‚Äî Tell the agent what to do in plain English and watch it happen in Unreal Engine.
- üîå **Model Context Protocol** ‚Äî Clean, standardized tool interface via FastMCP.
- ü§ñ **Multi-Model Support** ‚Äî Swap between **Groq**, **Ollama** (local), and **Google Gemini** with a single CLI flag.
- üí¨ **Interactive Mode** ‚Äî Chat with your Unreal scene in a REPL-style loop.
- üß© **Modular & Extensible** ‚Äî Adding a new tool is as simple as writing a decorated function.
- ‚ö° **WebSocket Transport** ‚Äî Real-time communication with Unreal Engine's Remote Control API.

---

## üöÄ Quick Start

### Prerequisites

- **Python 3.10+**
- **Unreal Engine 5.x** with the **Remote Control API** plugin enabled
- At least one LLM provider set up (see [Supported LLM Backends](#-supported-llm-backends))

### 1. Clone & Install

```bash
git clone https://github.com/Deepesh70/Unreal-MCP.git
cd Unreal-MCP
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
```

Edit `.env` and add the API key(s) for your chosen LLM provider:

```env
# Only fill in the provider you plan to use
GROQ_API_KEY=your_groq_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
# Ollama needs no API key ‚Äî just make sure it's running locally
```

### 3. Enable Remote Control in Unreal Engine

1. Open your Unreal Engine project.
2. Go to **Edit ‚Üí Plugins** and enable **Remote Control API**.
3. Restart the editor ‚Äî it will start listening on `ws://127.0.0.1:30020` by default.

### 4. Start the MCP Server

```bash
python server.py
```

The server starts on `http://localhost:8000` with SSE transport.

### 5. Run the Agent

```bash
# Full demo ‚Äî spawns actors, lists them, scales them
python agent.py groq

# Quick test ‚Äî just lists actors (1 API call)
python agent.py gemini --test

# Interactive chat mode
python agent.py groq --interactive

# Custom prompt
python agent.py gemini --prompt "spawn a sphere at 0 0 500"
```

---

## ü§ñ Supported LLM Backends

| Backend | Provider | Models | API Key Required |
|---------|----------|--------|------------------|
| `groq` | [Groq Cloud](https://console.groq.com/) | `llama-3.1-8b-instant` (default), `llama-3.3-70b-versatile` | ‚úÖ `GROQ_API_KEY` |
| `gemini` | [Google AI Studio](https://aistudio.google.com/apikey) | `gemini-2.5-pro` (default), `gemini-2.5-flash`, `gemini-2.0-flash` | ‚úÖ `GOOGLE_API_KEY` |
| `ollama` | [Ollama](https://ollama.com/) (local) | `llama3.3:70b` (default), `qwen2.5:72b`, `deepseek-r1:70b`, `command-r-plus:104b` | ‚ùå Local |

You can also run each backend directly:

```bash
python -m agents.groq_agent
python -m agents.gemini_agent --model gemini-2.5-flash
python -m agents.ollama_agent --model qwen2.5:72b

# List available models for a backend
python -m agents.gemini_agent --list-models
python -m agents.ollama_agent --list-models
```

---

## üõ†Ô∏è Available Tools

These MCP tools are automatically registered and available to the agent:

| Tool | Description | Example |
|------|-------------|---------|
| `spawn_actor` | Spawn an actor or shape at a given position | `"spawn a cube at 0 0 200"` |
| `list_actors` | List all actors in the current level | `"list all actors"` |
| `set_actor_scale` | Scale an actor using its full path | `"scale the cube to 5x"` |

**Supported spawn types:** `cube`, `sphere`, `cone`, `cylinder`, `plane`, `pointlight`, `spotlight`

---

## üèóÔ∏è Architecture

```
Unreal-MCP/
‚îú‚îÄ‚îÄ server.py                  ‚Üê Entry point ‚Äî starts the MCP server
‚îú‚îÄ‚îÄ agent.py                   ‚Üê CLI launcher for the LLM agent
‚îú‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ unreal_mcp/                ‚Üê Core MCP server package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            ‚Üê Creates shared FastMCP instance
‚îÇ   ‚îú‚îÄ‚îÄ config/                ‚Üê Settings & constants (WebSocket URL, ports)
‚îÇ   ‚îú‚îÄ‚îÄ connection/            ‚Üê WebSocket transport to Unreal Engine
‚îÇ   ‚îú‚îÄ‚îÄ mappings/              ‚Üê Friendly name ‚Üí UE asset/class path lookups
‚îÇ   ‚îú‚îÄ‚îÄ tools/                 ‚Üê MCP tool definitions (spawn, list, transform)
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 ‚Üê Response parsing & formatting helpers
‚îÇ
‚îú‚îÄ‚îÄ agents/                    ‚Üê LLM backend integrations
‚îÇ   ‚îú‚îÄ‚îÄ base.py                ‚Üê Shared agent runner (MCP connection + execution)
‚îÇ   ‚îú‚îÄ‚îÄ groq_agent.py          ‚Üê Groq Cloud backend
‚îÇ   ‚îú‚îÄ‚îÄ ollama_agent.py        ‚Üê Local Ollama backend
‚îÇ   ‚îî‚îÄ‚îÄ gemini_agent.py        ‚Üê Google Gemini backend
‚îÇ
‚îî‚îÄ‚îÄ docs/                      ‚Üê Detailed documentation
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md         ‚Üê Full architecture & dependency diagram
    ‚îú‚îÄ‚îÄ FLOW_SPAWN.md           ‚Üê Spawn actor flow walkthrough
    ‚îú‚îÄ‚îÄ FLOW_LIST.md            ‚Üê List actors flow walkthrough
    ‚îú‚îÄ‚îÄ FLOW_TRANSFORM.md       ‚Üê Transform flow walkthrough
    ‚îú‚îÄ‚îÄ AGENTS.md               ‚Üê Agent system documentation
    ‚îî‚îÄ‚îÄ ADDING_TOOLS.md         ‚Üê Guide to adding new MCP tools
```

### Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    SSE/HTTP     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   WebSocket    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Agent  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  MCP Server  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Unreal Engine   ‚îÇ
‚îÇ (LangChain) ‚îÇ    Port 8000    ‚îÇ  (FastMCP)   ‚îÇ   Port 30020   ‚îÇ (Remote Control) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

1. **Agent** receives a natural language prompt
2. **LangChain** plans which MCP tools to call
3. **MCP Server** translates tool calls into WebSocket commands
4. **Unreal Engine** executes the commands and returns results
5. **Agent** interprets the results and responds

---

## ‚öôÔ∏è Configuration

All server settings are centralized in `unreal_mcp/config/settings.py`:

| Setting | Default | Description |
|---------|---------|-------------|
| `UE_WS_URL` | `ws://127.0.0.1:30020` | Unreal Engine WebSocket endpoint |
| `SERVER_HOST` | `localhost` | MCP server bind address |
| `SERVER_PORT` | `8000` | MCP server port |
| `SERVER_TRANSPORT` | `sse` | MCP transport protocol |

---

## üß© Adding New Tools

Adding a new tool is straightforward. Create a new file in `unreal_mcp/tools/` and use the `@mcp.tool()` decorator:

```python
# unreal_mcp/tools/my_tool.py
from unreal_mcp import mcp
from unreal_mcp.connection import send_ue_ws_command

@mcp.tool()
async def my_new_tool(param: str) -> str:
    """Description of what this tool does."""
    response = await send_ue_ws_command(
        object_path="/Script/...",
        function_name="SomeFunction",
        parameters={"Param": param},
    )
    return f"Done: {response}"
```

Then import it in `unreal_mcp/tools/__init__.py`. See [`docs/ADDING_TOOLS.md`](docs/ADDING_TOOLS.md) for a full guide.

---

## üìö Documentation

| Document | Description |
|----------|-------------|
| [`ARCHITECTURE.md`](docs/ARCHITECTURE.md) | Full project architecture & dependency diagram |
| [`FLOW_SPAWN.md`](docs/FLOW_SPAWN.md) | End-to-end spawn actor flow |
| [`FLOW_LIST.md`](docs/FLOW_LIST.md) | End-to-end list actors flow |
| [`FLOW_TRANSFORM.md`](docs/FLOW_TRANSFORM.md) | End-to-end transform/scale flow |
| [`AGENTS.md`](docs/AGENTS.md) | Agent system & multi-model setup |
| [`ADDING_TOOLS.md`](docs/ADDING_TOOLS.md) | Step-by-step guide to adding new tools |

---

## üìã Requirements

```
fastmcp                      # MCP server framework
websockets                   # WebSocket connection to UE
langchain                    # Agent orchestration
langchain_core               # Core LangChain types
langchain_mcp_adapters       # MCP ‚Üî LangChain bridge
langchain_groq               # Groq Cloud provider
langchain_ollama             # Local Ollama provider
langchain_google_genai       # Google Gemini provider
python-dotenv                # Environment variable loading
```

Install everything with:

```bash
pip install -r requirements.txt
```

---

## ü§ù Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-tool`)
3. Add your tool following the [Adding Tools guide](docs/ADDING_TOOLS.md)
4. Commit your changes (`git commit -m 'Add amazing-tool'`)
5. Push to the branch (`git push origin feature/amazing-tool`)
6. Open a Pull Request

---

## üìù License

This project is open source. See the repository for license details.

---

<p align="center">
  Built with ‚ù§Ô∏è using <a href="https://modelcontextprotocol.io/">MCP</a>, <a href="https://python.langchain.com/">LangChain</a>, and <a href="https://www.unrealengine.com/">Unreal Engine</a>
</p>
