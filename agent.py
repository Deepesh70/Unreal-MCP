"""
Unreal MCP Agent â€” Multi-Model CLI Launcher.

Run with one of three LLM backends:

    python agent.py groq                    â† Full demo prompt
    python agent.py gemini --test           â† Quick test (1 API call only)
    python agent.py groq --interactive      â† Chat mode (type commands)

Or run a backend directly:

    python -m agents.groq_agent
    python -m agents.ollama_agent --model qwen2.5:72b
    python -m agents.gemini_agent --model gemini-2.5-flash
"""

import asyncio
import sys


def print_usage():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ®  Unreal MCP Agent Launcher  ğŸ®              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘  Usage:  python agent.py <backend> [options]                 â•‘
â•‘                                                              â•‘
â•‘  Backends:                                                   â•‘
â•‘    groq     Groq Cloud  â€” Llama 3.3 70B (fast, free tier)    â•‘
â•‘    ollama   Local       â€” 70B+ models on your GPU            â•‘
â•‘    gemini   Google      â€” Gemini 2.5 Pro (100B+ estimated)   â•‘
â•‘                                                              â•‘
â•‘  Options:                                                    â•‘
â•‘    --test          Quick test (1 API call, lists actors)      â•‘
â•‘    --interactive   Chat mode (type commands one by one)       â•‘
â•‘    --prompt "..."  Custom prompt                              â•‘
â•‘                                                              â•‘
â•‘  Examples:                                                   â•‘
â•‘    python agent.py groq                                      â•‘
â•‘    python agent.py gemini --test                              â•‘
â•‘    python agent.py groq --interactive                         â•‘
â•‘    python agent.py groq --prompt "spawn a cube at 0 0 200"   â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


def parse_options():
    """Parse --test, --interactive, and --prompt flags."""
    test_mode = "--test" in sys.argv
    interactive = "--interactive" in sys.argv or "-i" in sys.argv
    prompt = None

    if "--prompt" in sys.argv:
        idx = sys.argv.index("--prompt")
        if idx + 1 < len(sys.argv):
            prompt = sys.argv[idx + 1]

    return test_mode, interactive, prompt


async def main():
    if len(sys.argv) < 2 or sys.argv[1].startswith("-"):
        print_usage()
        sys.exit(1)

    backend = sys.argv[1].lower()
    test_mode, interactive, custom_prompt = parse_options()

    # Determine prompt
    from agents.base import run_agent, TEST_PROMPT
    if test_mode:
        prompt = TEST_PROMPT
        interactive = False
    else:
        prompt = custom_prompt  # None = use DEFAULT_PROMPT

    if backend == "groq":
        from agents.groq_agent import create_llm
        llm = create_llm()
        label = "Llama 3.1 8B via Groq (free tier)"

    elif backend == "ollama":
        from agents.ollama_agent import create_llm
        llm = create_llm()
        label = "llama3.3:70b via Ollama (local)"

    elif backend == "gemini":
        from agents.gemini_agent import create_llm
        llm = create_llm()
        label = "gemini-2.5-pro via Google Gemini"

    else:
        print(f"âŒ Unknown backend: '{backend}'")
        print_usage()
        sys.exit(1)

    await run_agent(llm, model_label=label, prompt=prompt, interactive=interactive)


if __name__ == "__main__":
    asyncio.run(main())